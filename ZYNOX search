

import os
import time
import json
import re
import webbrowser
from urllib.parse import urlparse
from collections import defaultdict
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import tldextract

load_dotenv()
# File paths
SERVICE_FILE = r"D:\zynox_agi\zynox_service.json"
CLOUD_MEMORY_FILE = r"D:\zynox_agi\zynox_cloud_encrypted.bin"

SERPAPI_KEY = os.getenv("SERPAPI_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
CLOUD_API_URL = os.getenv("CLOUD_API_URL")

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; Zynox/1.0)"}
SERPAPI_URL = "https://serpapi.com/search"

if not SERPAPI_KEY or not OPENROUTER_API_KEY or not CLOUD_API_URL:
    print("Missing environment variables. Please set SERPAPI_KEY, OPENROUTER_API_KEY, CLOUD_API_URL.")
    raise SystemExit(1)


# -------------------------
# Utilities
# -------------------------
def safe_request(url: str, params: dict = None, timeout: int = 10):
    """Safely makes an HTTP GET request and handles exceptions."""
    try:
        r = requests.get(url, params=params, headers=HEADERS, timeout=timeout)
        r.raise_for_status()
        return r
    except Exception as e:
        print(f"[safe_request] Failed: {e} -> {url}")
        return None


def post_json(url: str, payload: dict, timeout: int = 8):
    """Posts JSON data to a given URL."""
    try:
        r = requests.post(url, json=payload, timeout=timeout)
        r.raise_for_status()
        return r.json() if r.text else {}
    except Exception as e:
        print(f"[post_json] Failed to post to {url}: {e}")
        return None


# -------------------------
# SerpAPI queries
# -------------------------
def serpapi_search(query: str, num: int = 10) -> dict:
    """Performs a Google search using SerpAPI."""
    params = {
        "q": query,
        "api_key": SERPAPI_KEY,
        "engine": "google",
        "num": num,
    }
    resp = safe_request(SERPAPI_URL, params=params)
    if resp is None:
        return {}
    try:
        return resp.json()
    except Exception:
        return {}


def serpapi_images_for_query(query: str, num: int = 10) -> List[str]:
    """Retrieves image URLs for a given search query."""
    data = serpapi_search(query, num=10)
    images = []
    if not data:
        return images
    for key in ("images_results", "inline_images", "top_images", "image_results"):
        arr = data.get(key) or []
        if isinstance(arr, list) and arr:
            for el in arr[:num]:
                if isinstance(el, str):
                    images.append(el)
                elif isinstance(el, dict):
                    for f in ("original", "thumbnail", "link", "source"):
                        if el.get(f):
                            images.append(el[f])
                            break
            if images:
                break
    kg = data.get("knowledge_graph") or {}
    if not images and kg.get("image"):
        images.append(kg["image"])
    return images[:num]


# -------------------------
# Scraping & extraction
# -------------------------
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}", re.I)
SOCIAL_PATTERNS = {
    "LinkedIn": re.compile(r"https?://(?:www\.)?linkedin\.com/[^\s'\"<>]+", re.I),
    "Twitter": re.compile(r"https?://(?:www\.)?twitter\.com/[^\s'\"<>]+", re.I),
    "Instagram": re.compile(r"https?://(?:www\.)?instagram\.com/[^\s'\"<>]+", re.I),
    "Facebook": re.compile(r"https?://(?:www\.)?facebook\.com/[^\s'\"<>]+", re.I),
    "YouTube": re.compile(r"https?://(?:www\.)?youtube\.com/[^\s'\"<>]+", re.I),
    "TikTok": re.compile(r"https?://(?:www\.)?tiktok\.com/[^\s'\"<>]+", re.I),
}

PHONE_RE = re.compile(r"(?:\+?\d{1,3}[-.\s]?)?(?:\(?\d{2,4}\)?[-.\s]?)?\d{3,4}[-.\s]?\d{3,4}")


def scrape_page_text(url: str, max_chars: int = 4000) -> str:
    """Scrapes clean text from a webpage."""
    r = safe_request(url, timeout=8)
    if not r:
        return ""
    try:
        soup = BeautifulSoup(r.text, "html.parser")
        for s in soup(["script", "style", "noscript"]):
            s.decompose()
        text = " ".join(p.get_text(separator=" ", strip=True) for p in soup.find_all(["p", "li", "h1", "h2"]))
        return text[:max_chars]
    except Exception:
        return ""


def extract_emails_socials_phones(text: str) -> dict:
    """Extracts contact info and social links from text."""
    found = {"emails": set(), "socials": defaultdict(set), "phones": set()}
    if not text:
        return {"emails": [], "socials": {}, "phones": []}
    for m in EMAIL_RE.findall(text):
        found["emails"].add(m.strip())
    for ph in PHONE_RE.findall(text):
        ph_clean = re.sub(r"[^\d+]", "", ph)
        if 7 <= len(re.sub(r"\D", "", ph_clean)) <= 15:
            found["phones"].add(ph_clean)
    for name, pat in SOCIAL_PATTERNS.items():
        for match in pat.findall(text):
            found["socials"][name].add(match.strip())
    for at in re.findall(r"@([A-Za-z0-9_]{3,30})", text):
        found["socials"]["handles"].add("@" + at)
    return {
        "emails": sorted(found["emails"]),
        "socials": {k: sorted(v) for k, v in found["socials"].items()},
        "phones": sorted(found["phones"])
    }


# -------------------------
# OpenRouter summarization
# -------------------------
def openrouter_summarize(text: str,
                         system: str = "You are Zynox, an assistant that synthesizes internet findings into a clean profile.",
                         user_prompt: str = "Summarize the key facts and list likely social accounts, occupations, and notable works."):
    """Uses OpenRouter API to summarize text with an AI model."""
    try:
        url = "https://openrouter.ai/api/v1/chat/completions"
        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": system},
                {"role": "user", "content": f"{user_prompt}\n\n{text}"}
            ],
            "temperature": 0.1,
            "max_tokens": 600
        }
        headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
        r = requests.post(url, json=payload, headers=headers, timeout=15)
        r.raise_for_status()
        out = r.json()
        choices = out.get("choices") or []
        if choices and isinstance(choices, list):
            msg = choices[0].get("message") or choices[0].get("text") or ""
            if isinstance(msg, dict):
                return msg.get("content", "").strip()
            return str(msg).strip()
        return ""
    except Exception as e:
        print(f"[openrouter_summarize] Error: {e}")
        return ""


# -------------------------
# New Feature: URL Reading
# -------------------------
def read_and_summarize_url(url: str, max_chars: int = 5000) -> Optional[dict]:
    """Fetches, scrapes, and summarizes the content of a given URL."""
    print(f"[+] Fetching content from: {url}")
    try:
        # Step 1: Fetch the content of the URL.
        response = safe_request(url)
        if not response:
            print(f"[-] Failed to retrieve content from {url}.")
            return None

        # Step 2: Scrape the text from the HTML.
        soup = BeautifulSoup(response.text, "html.parser")
        # Remove script, style, and noscript tags to get clean text.
        for s in soup(["script", "style", "noscript"]):
            s.decompose()
        # Join the text from paragraphs, list items, and headings.
        text = " ".join(
            p.get_text(separator=" ", strip=True) for p in soup.find_all(["p", "li", "h1", "h2", "h3", "title"]))

        # Limit the text length to avoid excessive token usage for summarization.
        clean_text = text[:max_chars]

        if not clean_text.strip():
            print("[-] No significant text content found on the page.")
            return None

        # Step 3: Use OpenRouter to summarize the scraped text.
        user_prompt = "Provide a concise summary of the key information from this webpage. What is the main purpose of the page? What are the key points?"
        summary = openrouter_summarize(clean_text, user_prompt=user_prompt)

        return {
            "url": url,
            "summary": summary
        }

    except Exception as e:
        print(f"[!] An error occurred while processing {url}: {e}")
        return None


# -------------------------
# Aggregation and profile builder
# -------------------------
def build_person_profile(name: str, max_links: int = 8) -> dict:
    """Builds a comprehensive profile for a person."""
    query = name.strip()
    print(f"[+] Searching for: {query}")
    serp = serpapi_search(query, num=max_links)
    profile = {
        "query": query,
        "timestamp": int(time.time()),
        "knowledge_graph": {},
        "organic_results": [],
        "top_images": [],
        "aggregated_socials": {},
        "emails": [],
        "phones": [],
        "scraped_snippets": {},
        "summary": ""
    }
    kg = serp.get("knowledge_graph") or {}
    if kg:
        profile["knowledge_graph"] = kg
    organics = serp.get("organic_results") or []
    for i, r in enumerate(organics[:max_links]):
        title = r.get("title") or r.get("position") or ""
        link = r.get("link") or r.get("url") or r.get("cached_page_url") or ""
        snippet = r.get("snippet") or r.get("rich_snippet", {}).get("top", {}).get("snippet", "")
        profile["organic_results"].append({"title": title, "link": link, "snippet": snippet})
    images = serpapi_images_for_query(query, num=12)
    profile["top_images"] = images
    aggregated_socials = defaultdict(set)
    aggregated_emails = set()
    aggregated_phones = set()
    for entry in profile["organic_results"][:max_links]:
        url = entry.get("link")
        if not url:
            continue
        text = scrape_page_text(url, max_chars=5000)
        if not text:
            continue
        profile["scraped_snippets"][url] = text[:1000]
        ext = extract_emails_socials_phones(text)
        for e in ext.get("emails", []):
            aggregated_emails.add(e)
        for ph in ext.get("phones", []):
            aggregated_phones.add(ph)
        for sname, svals in ext.get("socials", {}).items():
            for v in svals:
                aggregated_socials[sname].add(v)
        time.sleep(0.2)
    profile["aggregated_socials"] = {k: sorted(list(v)) for k, v in aggregated_socials.items()}
    profile["emails"] = sorted(list(aggregated_emails))
    profile["phones"] = sorted(list(aggregated_phones))
    kg_text = " ".join([str(kg.get(k, "")) for k in kg.keys()]) if kg else ""
    extkg = extract_emails_socials_phones(kg_text)
    for e in extkg.get("emails", []):
        if e not in profile["emails"]:
            profile["emails"].append(e)
    for sname, svals in extkg.get("socials", {}).items():
        profile["aggregated_socials"].setdefault(sname, [])
        for v in svals:
            if v not in profile["aggregated_socials"][sname]:
                profile["aggregated_socials"][sname].append(v)
    pieces = []
    if kg:
        pieces.append(json.dumps(kg))
    for entry in profile["organic_results"][:5]:
        if entry.get("snippet"):
            pieces.append(entry["snippet"])
    for url, snip in profile["scraped_snippets"].items():
        pieces.append(snip[:1000])
    combined = "\n\n".join(pieces)[:6000]
    if combined.strip():
        summary = openrouter_summarize(combined,
                                       user_prompt=f"Create a clear, structured profile for '{name}'. Include: likely occupations, education, awards, social accounts (with URLs), email addresses, and relevant links. Format as bullet points + short paragraph.")
        profile["summary"] = summary
    try:
        save_payload = {"owner_id": None, "data": {"type": "person_profile", "query": name, "profile": profile},
                        "tags": ["person_profile"]}
        _ = post_json(CLOUD_API_URL, save_payload)
    except Exception as e:
        print(f"[!] Cloud save failed: {e}")
    return profile


# -------------------------
# Nice console presentation
# -------------------------
def pretty_print_profile(profile: dict):
    """Prints the person profile in a clean format."""
    print("\n" + "=" * 80)
    print(f"Zynox Profile: {profile.get('query')}")
    if profile.get("knowledge_graph"):
        print("\n[Knowledge Card]")
        kg = profile["knowledge_graph"]
        for k in ("title", "description", "type", "name"):
            if kg.get(k):
                print(f" - {k}: {kg.get(k)}")
    print("\n[Top Links]")
    for idx, r in enumerate(profile.get("organic_results", [])[:10], 1):
        print(f"{idx}. {r.get('title')[:120]}")
        print(f"   {r.get('link')}")
        if r.get("snippet"):
            print(f"   snippet: {r.get('snippet')[:200]}")
    if profile.get("aggregated_socials"):
        print("\n[Social Accounts Found]")
        for k, vals in profile["aggregated_socials"].items():
            print(f" - {k}:")
            for v in vals[:8]:
                print(f"    {v}")
    if profile.get("emails"):
        print("\n[Emails]")
        for e in profile["emails"]:
            print(" -", e)
    if profile.get("phones"):
        print("\n[Phones]")
        for p in profile["phones"]:
            print(" -", p)
    if profile.get("top_images"):
        print("\n[Image URLs]")
        for i, u in enumerate(profile["top_images"][:8], 1):
            print(f" {i}. {u}")
    if profile.get("summary"):
        print("\n[Summarized Profile]")
        print(profile["summary"][:2000])
    print("\n" + "=" * 80)


# -------------------------
# CLI
# -------------------------
def main_loop():
    """Main command-line interface loop."""
    print("===== Zynox PERSON SEARCH & URL READER =====")
    print("Commands:")
    print(" - profile <person name>   : search everything about a person")
    print(" - read <url>              : fetch and summarize the content of a URL")
    print(" - open <n>                : open nth top link in browser (from last person search)")
    print(" - openimg <n>             : open nth image URL in browser")
    print(" - exit                    : quit\n")
    last_profile = None
    while True:
        cmd = input("zynox> ").strip()
        if not cmd:
            continue
        if cmd.lower() in ("exit", "quit"):
            break
        if cmd.startswith("profile "):
            name = cmd[len("profile "):].strip()
            last_profile = build_person_profile(name)
            pretty_print_profile(last_profile)
            continue
        if cmd.startswith("read "):
            url = cmd[len("read "):].strip()
            parsed_url = urlparse(url)
            if parsed_url.scheme in ('http', 'https'):
                result = read_and_summarize_url(url)
                if result and result.get("summary"):
                    print("\n" + "=" * 80)
                    print(f"Summary for: {result['url']}")
                    print("\n" + result['summary'])
                    print("=" * 80)
                elif result:
                    print(f"No summary could be generated for {url}.")
            else:
                print("Invalid URL format. Please provide a full URL starting with http:// or https://")
            continue
        if cmd.startswith("openimg ") and last_profile:
            try:
                n = int(cmd.split()[1]) - 1
                url = last_profile.get("top_images", [])[n]
                print(f"Opening image: {url}")
                webbrowser.open(url, new=2)
            except Exception as e:
                print("Invalid image index or no last profile:", e)
            continue
        if cmd.startswith("open ") and last_profile:
            try:
                n = int(cmd.split()[1]) - 1
                url = last_profile.get("organic_results", [])[n]["link"]
                print(f"Opening link: {url}")
                webbrowser.open(url, new=2)
            except Exception as e:
                print("Invalid index or no last profile:", e)
            continue
        print("Unknown command. Try 'profile <name>', 'read <url>', or 'exit'.")


if __name__ == "__main__":
    main_loop()
